{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/python/3.12.1/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in /usr/local/python/3.12.1/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: transformers in /usr/local/python/3.12.1/lib/python3.12/site-packages (4.48.3)\n",
      "Requirement already satisfied: datasets in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.2.0)\n",
      "Requirement already satisfied: nltk in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (2.2.0)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: click in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio transformers datasets nltk numpy pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter, defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1432\n",
      "Testing samples: 359\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Correct_Word</th>\n",
       "      <th>Misspelled_Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?</td>\n",
       "      <td>Consulatoin 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?</td>\n",
       "      <td>Doc_chm 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?</td>\n",
       "      <td>Half-hose 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>?</td>\n",
       "      <td>Hurbured 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>?</td>\n",
       "      <td>anetes 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Correct_Word Misspelled_Word\n",
       "0            ?   Consulatoin 1\n",
       "1            ?       Doc_chm 1\n",
       "2            ?     Half-hose 1\n",
       "3            ?      Hurbured 1\n",
       "4            ?        anetes 1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "with open(\"holbrook-missp.dat\", \"r\") as file:\n",
    "    raw_data = file.readlines()\n",
    "\n",
    "# Process data\n",
    "corrections = {}\n",
    "current_correct_word = None\n",
    "\n",
    "for line in raw_data:\n",
    "    word = line.strip()\n",
    "    if word.startswith(\"$\"):  # Correct words start with $\n",
    "        current_correct_word = word[1:]\n",
    "        corrections[current_correct_word] = []\n",
    "    else:\n",
    "        if current_correct_word:\n",
    "            corrections[current_correct_word].append(word)\n",
    "\n",
    "# Convert into a DataFrame\n",
    "df = pd.DataFrame([(k, v) for k, values in corrections.items() for v in values],\n",
    "                  columns=[\"Correct_Word\", \"Misspelled_Word\"])\n",
    "\n",
    "# Split into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training samples:\", len(train_df))\n",
    "print(\"Testing samples:\", len(test_df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Function to calculate Levenshtein Distance\\ndef levenshtein_distance(s1, s2):\\n    if len(s1) < len(s2):\\n        return levenshtein_distance(s2, s1)\\n\\n    if len(s2) == 0:\\n        return len(s1)\\n\\n    previous_row = range(len(s2) + 1)\\n    for i, c1 in enumerate(s1):\\n        current_row = [i + 1]\\n        for j, c2 in enumerate(s2):\\n            insertions = previous_row[j + 1] + 1\\n            deletions = current_row[j] + 1\\n            substitutions = previous_row[j] + (c1 != c2)\\n            current_row.append(min(insertions, deletions, substitutions))\\n        previous_row = current_row\\n\\n    return previous_row[-1]\\n\\n# Function to find the closest correct word\\ndef find_closest_word(misspelled_word, correct_words):\\n    distances = [(word, levenshtein_distance(misspelled_word, word)) for word in correct_words]\\n    closest_word = min(distances, key=lambda x: x[1])[0]\\n    return closest_word\\n\\n# Evaluate the model on the test set\\ncorrect_words = train_df[\"Correct_Word\"].unique()\\ntest_df[\"Predicted_Correct_Word\"] = test_df[\"Misspelled_Word\"].apply(lambda x: find_closest_word(x, correct_words))\\n\\n# Calculate accuracy\\naccuracy = np.mean(test_df[\"Correct_Word\"] == test_df[\"Predicted_Correct_Word\"])\\nprint(\"Accuracy:\", accuracy)\\n\\ntest_df.head()'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Function to calculate Levenshtein Distance\n",
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]\n",
    "\n",
    "# Function to find the closest correct word\n",
    "def find_closest_word(misspelled_word, correct_words):\n",
    "    distances = [(word, levenshtein_distance(misspelled_word, word)) for word in correct_words]\n",
    "    closest_word = min(distances, key=lambda x: x[1])[0]\n",
    "    return closest_word\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "correct_words = train_df[\"Correct_Word\"].unique()\n",
    "test_df[\"Predicted_Correct_Word\"] = test_df[\"Misspelled_Word\"].apply(lambda x: find_closest_word(x, correct_words))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(test_df[\"Correct_Word\"] == test_df[\"Predicted_Correct_Word\"])\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "test_df.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Build n-gram frequency models\\ndef build_ngram_model(words, n=2):\\n    ngrams = [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\\n    ngram_freq = Counter(ngrams)\\n    return ngram_freq\\n\\ncorrect_words = train_df[\"Correct_Word\"].tolist()\\nunigram_freq = Counter(correct_words)\\nbigram_freq = build_ngram_model(correct_words, n=2)\\n\\n# Function to calculate word likelihood\\ndef word_likelihood(word, unigram_freq, bigram_freq):\\n    likelihood = unigram_freq[word] / sum(unigram_freq.values())\\n    for i in range(len(word)-1):\\n        bigram = (word[i], word[i+1])\\n        likelihood *= (bigram_freq[bigram] + 1) / (unigram_freq[word[i]] + len(unigram_freq))\\n    return likelihood\\n\\n# Function to find the most likely correct word\\ndef find_most_likely_word(misspelled_word, correct_words, unigram_freq, bigram_freq):\\n    likelihoods = [(word, word_likelihood(word, unigram_freq, bigram_freq)) for word in correct_words]\\n    most_likely_word = max(likelihoods, key=lambda x: x[1])[0]\\n    return most_likely_word\\n\\n# Evaluate the model on the test set\\ntest_df[\"Predicted_Correct_Word\"] = test_df[\"Misspelled_Word\"].apply(lambda x: find_most_likely_word(x, correct_words, unigram_freq, bigram_freq))\\n\\n# Calculate accuracy\\naccuracy = np.mean(test_df[\"Correct_Word\"] == test_df[\"Predicted_Correct_Word\"])\\nprint(\"Accuracy:\", accuracy)\\n\\ntest_df.head()'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Build n-gram frequency models\n",
    "def build_ngram_model(words, n=2):\n",
    "    ngrams = [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "    ngram_freq = Counter(ngrams)\n",
    "    return ngram_freq\n",
    "\n",
    "correct_words = train_df[\"Correct_Word\"].tolist()\n",
    "unigram_freq = Counter(correct_words)\n",
    "bigram_freq = build_ngram_model(correct_words, n=2)\n",
    "\n",
    "# Function to calculate word likelihood\n",
    "def word_likelihood(word, unigram_freq, bigram_freq):\n",
    "    likelihood = unigram_freq[word] / sum(unigram_freq.values())\n",
    "    for i in range(len(word)-1):\n",
    "        bigram = (word[i], word[i+1])\n",
    "        likelihood *= (bigram_freq[bigram] + 1) / (unigram_freq[word[i]] + len(unigram_freq))\n",
    "    return likelihood\n",
    "\n",
    "# Function to find the most likely correct word\n",
    "def find_most_likely_word(misspelled_word, correct_words, unigram_freq, bigram_freq):\n",
    "    likelihoods = [(word, word_likelihood(word, unigram_freq, bigram_freq)) for word in correct_words]\n",
    "    most_likely_word = max(likelihoods, key=lambda x: x[1])[0]\n",
    "    return most_likely_word\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_df[\"Predicted_Correct_Word\"] = test_df[\"Misspelled_Word\"].apply(lambda x: find_most_likely_word(x, correct_words, unigram_freq, bigram_freq))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(test_df[\"Correct_Word\"] == test_df[\"Predicted_Correct_Word\"])\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "test_df.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Build a model of common spelling mistakes\\ndef build_error_model(df):\\n    error_model = defaultdict(Counter)\\n    for _, row in df.iterrows():\\n        correct_word = row[\"Correct_Word\"]\\n        misspelled_word = row[\"Misspelled_Word\"]\\n        for i in range(min(len(correct_word), len(misspelled_word))):\\n            if correct_word[i] != misspelled_word[i]:\\n                error_model[correct_word[i]][misspelled_word[i]] += 1\\n    return error_model\\n\\nerror_model = build_error_model(train_df)\\n\\n# Function to calculate word probability given the noisy channel model\\ndef word_probability(word, correct_word, error_model):\\n    probability = 1.0\\n    for i in range(min(len(word), len(correct_word))):\\n        if word[i] != correct_word[i]:\\n            denominator = sum(error_model[correct_word[i]].values()) + len(error_model[correct_word[i]])\\n            if denominator == 0:\\n                denominator = 1  # Avoid dividing by zero\\n            probability *= (error_model[correct_word[i]][word[i]] + 1) / denominator\\n    return probability\\n\\n# Function to find the most likely correct word\\ndef find_most_likely_word(misspelled_word, correct_words, error_model):\\n    probabilities = [(word, word_probability(misspelled_word, word, error_model)) for word in correct_words]\\n    most_likely_word = max(probabilities, key=lambda x: x[1])[0]\\n    return most_likely_word\\n\\n# Evaluate the model on the test set\\ncorrect_words = train_df[\"Correct_Word\"].unique()\\ntest_df[\"Predicted_Correct_Word\"] = test_df[\"Misspelled_Word\"].apply(lambda x: find_most_likely_word(x, correct_words, error_model))\\n\\n# Calculate accuracy\\naccuracy = np.mean(test_df[\"Correct_Word\"] == test_df[\"Predicted_Correct_Word\"])\\nprint(\"Accuracy:\", accuracy)\\n\\ntest_df.head()'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Build a model of common spelling mistakes\n",
    "def build_error_model(df):\n",
    "    error_model = defaultdict(Counter)\n",
    "    for _, row in df.iterrows():\n",
    "        correct_word = row[\"Correct_Word\"]\n",
    "        misspelled_word = row[\"Misspelled_Word\"]\n",
    "        for i in range(min(len(correct_word), len(misspelled_word))):\n",
    "            if correct_word[i] != misspelled_word[i]:\n",
    "                error_model[correct_word[i]][misspelled_word[i]] += 1\n",
    "    return error_model\n",
    "\n",
    "error_model = build_error_model(train_df)\n",
    "\n",
    "# Function to calculate word probability given the noisy channel model\n",
    "def word_probability(word, correct_word, error_model):\n",
    "    probability = 1.0\n",
    "    for i in range(min(len(word), len(correct_word))):\n",
    "        if word[i] != correct_word[i]:\n",
    "            denominator = sum(error_model[correct_word[i]].values()) + len(error_model[correct_word[i]])\n",
    "            if denominator == 0:\n",
    "                denominator = 1  # Avoid dividing by zero\n",
    "            probability *= (error_model[correct_word[i]][word[i]] + 1) / denominator\n",
    "    return probability\n",
    "\n",
    "# Function to find the most likely correct word\n",
    "def find_most_likely_word(misspelled_word, correct_words, error_model):\n",
    "    probabilities = [(word, word_probability(misspelled_word, word, error_model)) for word in correct_words]\n",
    "    most_likely_word = max(probabilities, key=lambda x: x[1])[0]\n",
    "    return most_likely_word\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "correct_words = train_df[\"Correct_Word\"].unique()\n",
    "test_df[\"Predicted_Correct_Word\"] = test_df[\"Misspelled_Word\"].apply(lambda x: find_most_likely_word(x, correct_words, error_model))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(test_df[\"Correct_Word\"] == test_df[\"Predicted_Correct_Word\"])\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "test_df.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellingDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.vocab = set(\"\".join(data[\"Misspelled_Word\"]) + \"\".join(data[\"Correct_Word\"]))\n",
    "        self.char2idx = {ch: i+1 for i, ch in enumerate(self.vocab)}  # +1 to reserve 0 for padding\n",
    "        self.idx2char = {i: ch for ch, i in self.char2idx.items()}\n",
    "        self.max_len = max(data[\"Misspelled_Word\"].apply(len))  # Max word length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        misspelled = [self.char2idx[ch] for ch in row[\"Misspelled_Word\"]]\n",
    "        correct = [self.char2idx[ch] for ch in row[\"Correct_Word\"]]\n",
    "        \n",
    "        # Pad sequences\n",
    "        misspelled += [0] * (self.max_len - len(misspelled))\n",
    "        correct += [0] * (self.max_len - len(correct))\n",
    "        \n",
    "        return torch.tensor(misspelled), torch.tensor(correct)\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = SpellingDataset(train_df)\n",
    "test_dataset = SpellingDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=32, hidden_dim=64):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim)  # +1 for padding\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size + 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.encoder(embedded)\n",
    "        out, _ = self.decoder(embedded, (hidden, torch.zeros_like(hidden)))\n",
    "        return self.fc(out)\n",
    "\n",
    "# Initialize model\n",
    "vocab_size = len(train_dataset.vocab)\n",
    "model = Seq2SeqModel(vocab_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    misspelled, correct = zip(*batch)\n",
    "    misspelled = pad_sequence(misspelled, batch_first=True, padding_value=0)\n",
    "    correct = pad_sequence(correct, batch_first=True, padding_value=0)\n",
    "    return misspelled, correct\n",
    "\n",
    "batch_size = 32  # Define batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 4.219175338745117\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 4.183636665344238\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 4.135247230529785\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 4.089835166931152\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 4.0490827560424805\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 4.003973960876465\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 3.9698233604431152\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 3.921349287033081\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 3.845768690109253\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 3.796046495437622\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 3.7512917518615723\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 18])\n",
      "Loss: 3.705265760421753\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 3.6344716548919678\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 3.564587354660034\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 3.540616512298584\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 3.459203004837036\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 3.3349716663360596\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 3.279067039489746\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 3.143171787261963\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 3.1541025638580322\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 3.062457323074341\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 2.8400704860687256\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 2.9511282444000244\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 2.7892136573791504\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 2.6357884407043457\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 2.631364107131958\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 2.485684871673584\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 2.4903368949890137\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 2.353238344192505\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 2.1935648918151855\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 2.2461090087890625\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 2.1893327236175537\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 1.928860068321228\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 2.0189685821533203\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 1.878408432006836\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 1.7466713190078735\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 2.0204079151153564\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 1.721081256866455\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 1.6641274690628052\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 1.7683521509170532\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 1.7344467639923096\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 1.5727914571762085\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 1.668179988861084\n",
      "Output shape: torch.Size([32, 15, 64])\n",
      "Target shape: torch.Size([32, 15])\n",
      "Loss: 1.7999166250228882\n",
      "Output shape: torch.Size([24, 15, 64])\n",
      "Target shape: torch.Size([24, 15])\n",
      "Loss: 1.9675923585891724\n"
     ]
    }
   ],
   "source": [
    "for misspelled, correct in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = model(misspelled)  # Forward pass\n",
    "    \n",
    "    # Debugging shape mismatch issue\n",
    "    print(f\"Output shape: {outputs.shape}\")  # Expected: (batch_size, seq_len, vocab_size)\n",
    "    print(f\"Target shape: {correct.shape}\")  # Expected: (batch_size, seq_len)\n",
    "\n",
    "    batch_size, seq_len, vocab_size = outputs.shape  # Unpack output dimensions\n",
    "\n",
    "    # Ensure the correct tensor is the same shape as the output tensor\n",
    "    correct = correct[:, :seq_len]\n",
    "\n",
    "    # Reshape tensors correctly for loss computation\n",
    "    loss = criterion(\n",
    "        outputs.reshape(batch_size * seq_len, vocab_size),  # Flatten output for cross-entropy\n",
    "        correct.reshape(-1)  # Flatten target labels\n",
    "    )\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Loss: {loss.item()}\")  # Monitor loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misspelled: PddoT\n",
      "Predicted: \n",
      "Correct: Pddg0m\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for misspelled, correct in test_loader:\n",
    "        outputs = model(misspelled)\n",
    "        predictions = torch.argmax(outputs, dim=-1)\n",
    "        print(\"Misspelled:\", \"\".join([train_dataset.idx2char[idx.item()] for idx in misspelled[0] if idx.item() != 0]))\n",
    "        print(\"Predicted:\", \"\".join([train_dataset.idx2char[idx.item()] for idx in predictions[0] if idx.item() != 0]))\n",
    "        print(\"Correct:\", \"\".join([train_dataset.idx2char[idx.item()] for idx in correct[0] if idx.item() != 0]))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_correct = 0\n",
    "total_count = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for misspelled, correct in test_loader:\n",
    "        outputs = model(misspelled)\n",
    "        predictions = torch.argmax(outputs, dim=-1)\n",
    "        \n",
    "        # Flatten the tensors for comparison\n",
    "        correct_flat = correct.view(-1)\n",
    "        predictions_flat = predictions.view(-1)\n",
    "        \n",
    "        # Count non-padding tokens\n",
    "        non_padding = correct_flat != 0\n",
    "        \n",
    "        # Calculate the number of correct predictions\n",
    "        total_correct += (predictions_flat[non_padding] == correct_flat[non_padding]).sum().item()\n",
    "        total_count += non_padding.sum().item()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = total_correct / total_count\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Convert words to token IDs\n",
    "train_df[\"tokenized_misspelled\"] = train_df[\"Misspelled_Word\"].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "train_df[\"tokenized_correct\"] = train_df[\"Correct_Word\"].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT\n",
    "bert_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model.train()\n",
    "\n",
    "optimizer = optim.Adam(bert_model.parameters(), lr=5e-5)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "def tokenize_and_pad(texts, tokenizer, max_length):\n",
    "    tokenized_texts = [tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=max_length) for text in texts]\n",
    "    padded_texts = pad_sequence([torch.tensor(text) for text in tokenized_texts], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    return padded_texts\n",
    "\n",
    "# Prepare inputs for training\n",
    "max_length = 64 # Reduced max_length to a more typical value for BERT\n",
    "input_ids = tokenize_and_pad(train_df[\"Misspelled_Word\"].tolist(), tokenizer, max_length=max_length)\n",
    "labels = tokenize_and_pad(train_df[\"Correct_Word\"].tolist(), tokenizer, max_length=max_length)\n",
    "\n",
    "# Ensure both input_ids and labels have the same shape\n",
    "# by truncating to the minimum length if necessary\n",
    "min_length = min(input_ids.shape[1], labels.shape[1])\n",
    "input_ids = input_ids[:, :min_length]\n",
    "labels = labels[:, :min_length]\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = bert_model(input_ids, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
